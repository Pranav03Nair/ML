{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63c042ab",
   "metadata": {},
   "source": [
    "## Neural Network - ReLU and Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef611a02",
   "metadata": {},
   "source": [
    "#### Description\n",
    "This Neural Network Model is made usind ReLU Activation Function and Adam Optimization Function\n",
    "\n",
    "The intention of this MODEL is to utilize Binary Classification using basic Feedforward Neural Network\n",
    "\n",
    "Note : A **binary classification problem** is a type of predictive modeling problem where the goal is to **predict one of two possible outcomes** for each input instance. The two possible outcomes are typically referred to as classes, and are often represented by the values 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a54ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (1.24.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (66.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\20pra\\anaconda3\\envs\\aiml\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1dc6d7b",
   "metadata": {},
   "source": [
    "#### Step 1: Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f75e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6003ace1",
   "metadata": {},
   "source": [
    "**NumPy** is a Python library that provides a high-performance multidimensional array object. It is the fundamental package for scientific computing in Python.\n",
    "**Keras** is a high-level neural networks API. Keras provides a simple and intuitive API for defining neural networks, as well as a wide range of tools for training and evaluating networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0435d38a",
   "metadata": {},
   "source": [
    "#### Step 2: Load and Preprocess the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7155c718",
   "metadata": {},
   "source": [
    "We will use the breast cancer binary classification data pre-provided in sklearn. Each sample has only two classifications - malignant(1) or benign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6750ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e6cd1a5",
   "metadata": {},
   "source": [
    "##### Import Explaintations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09e86d5c",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "This line imports test_split module that divides the dataset into TRAINING SET and TEST SET.\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "This line imports the StandardScaler() class from the sklearn.preprocessing module. This class is used to standardize the features of a dataset.\n",
    " \n",
    " **Mean:** The mean is the average of all the values in a dataset. It is calculated by adding up all the values and dividing by the number of values.\n",
    " \n",
    " **Standard deviation:** The standard deviation is a measure of how spread out the values in a dataset are. It is calculated by taking the square root of the variance. The variance is calculated by taking the average of the squared differences between each value in the dataset and the mean.\n",
    "\n",
    "Standardization is a process of transforming the features of a dataset so that they have a mean of 0 and a standard deviation of 1. This makes the features more comparable to each other, which can improve the performance of machine learning models.\n",
    "\n",
    "Eg to understand Mean, Std Deviation and Standardization -  A dataset with two features: age and height. \n",
    "\n",
    "  - The age feature has a mean of 30 and a standard deviation of 10.\n",
    "  Meaning values of the age feature are typically between 20 and 40.\n",
    "  And people can be any age, but they are typically between 0 and 100 years old.\n",
    "\n",
    "  - The height feature has a mean of 60 and a standard deviation of 15.\n",
    "  Meaning values of the age feature are typically between 45 and 75.\n",
    "  And people are typically between 50 and 80 inches.\n",
    "\n",
    "  - Even though Theoretically Std Deviation of height > age. The spread of AGE is greater since people normally do exist of ages 20 and 40. But people are not of height 50 inches or 80 inches.\n",
    "\n",
    "If you don't standardize these features, the age feature will have a much larger range of values than the height feature. This can make it difficult for machine learning models to learn from the data. After standardizing the features, the age feature will have a mean of 0 and a standard deviation of 1. The height feature will still have a mean of 0 and a standard deviation of 1.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7f381c",
   "metadata": {},
   "source": [
    "#### Step 2: Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3ec93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "data = load_breast_cancer()\n",
    "    # X = Data = Features of the dataset samples\n",
    "    # Y = Target = Target values, here binary 1 and 0\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features to be between -1 and 1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aba851d",
   "metadata": {},
   "source": [
    "##### Train_test_split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aef047ea",
   "metadata": {},
   "source": [
    "```train_test_split()``` Function takes four arguments:\n",
    "\n",
    "- X:            The features of the dataset.\n",
    "- y:            The target variable.\n",
    "- test_size:    The percentage of the data that should be used for the testing set.\n",
    "- random_state: The seed for the random number generator.\n",
    "\n",
    "In this case, 20% of the data will be used for the testing set. The random_state parameter is set to 42, which ensures that the data is split in the same way each time the code is run.\n",
    "\n",
    "The line of code splits the dataset into four variables:\n",
    "\n",
    "- X_train: The training features.\n",
    "- X_test: The testing features.\n",
    "- y_train: The training target variable.\n",
    "- y_test: The testing target variable.\n",
    "\n",
    "```random_state``` parameter is similar to Minecraft World seed, where the same seed generates the same World each time. Similarly, same random_state integer will ensure that the data split pattern is the same each time, irrespective of whether the ML dataset is the same or different.\n",
    "\n",
    "Why **random_state** ?\n",
    "Shuffling does not directly affect the proportion of splitting. However, it can indirectly affect the proportion of splitting by preventing the model from learning patterns in the order of the data.\n",
    "**For example,** let's say that we have a dataset of 100 points, and we want to split the data into a training set and a testing set with a 80/20 split. If we do not shuffle the data, then it is possible that the training set will contain all of the points from one class, and the testing set will contain all of the points from the other class. This will make it difficult for the model to learn to generalize to new data, and it will also make it difficult to evaluate the performance of the model.\n",
    "\n",
    "In machine learning, a class is a group of data points that share a common characteristic. For example, in a dataset of images of cats and dogs, the classes would be \"cat\" and \"dog\".\n",
    "So if shuffling was not done, it is possible that the training set would contain all of the points from the \"cat\" class, and the testing set would contain all of the points from the \"dog\" class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4060c708",
   "metadata": {},
   "source": [
    "This line normalizes the features so that they have a mean of 0 and a standard deviation of 1. This is important because it helps to improve the performance of machine learning models.\n",
    "The **fit_transform** method fits the scaler to the training data and then transforms the training data. The **transform** method then transforms the testing data using the scaler that was fit to the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb2bbf3",
   "metadata": {},
   "source": [
    "##### Fit_Transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "340df166",
   "metadata": {},
   "source": [
    "```\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "We know Standardization is a scaling method to scale the dataset to Mean = 0, Std Deviation = 1. Here, transform refers to Standardization only\n",
    "- **Fit** is used to learn the parameters of the transformation.\n",
    "- **Transform** is used to apply the transformation to the data.\n",
    "- **Fit_transform** is a combination of fit and transform. It learns the parameters of the transformation on the training data and then applies the transformation.\n",
    "\n",
    "!!! We only fit_transform the training data. This is because we want the model to learn the parameters of the transformation on the training data, and then apply the same transformation to the testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d88924f",
   "metadata": {},
   "source": [
    "#### Step 3: Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462c8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Add the first hidden layer with ReLU activation function\n",
    "model.add(keras.layers.Dense(30, activation='relu', input_shape=(30,)))\n",
    "\n",
    "# Add another hidden layer\n",
    "model.add(keras.layers.Dense(15, activation='relu'))\n",
    "\n",
    "# Add the output layer. Since this is a binary classification problem, we'll use the sigmoid activation function\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c79f76",
   "metadata": {},
   "source": [
    "1. `model = keras.models.Sequential()`: This line creates a new sequential model. The sequential model is a linear stack of layers, where you can easily add and remove layers. It is a common choice for building neural networks in Keras.\n",
    "\n",
    "2. `model.add(keras.layers.Dense(30, activation='relu', input_shape=(30,)))`: This line adds the first hidden layer to the model. The `Dense` layer represents a fully connected layer, where each neuron is connected to every neuron in the *previous* layer. The layer has 30 units/neurons. The `activation='relu'` argument sets the Rectified Linear Unit (ReLU) as the activation function for this layer. The `input_shape=(30,)` argument defines the shape of the input data expected by this layer. Since it is the first layer in the model, it expects input with 30 features.\n",
    "\n",
    "3. `model.add(keras.layers.Dense(15, activation='relu'))`: This line adds another hidden layer to the model. It is similar to the previous line but with 15 units/neurons instead of 30. The input shape is not specified explicitly here since it is automatically inferred from the previous layer's output shape.\n",
    "\n",
    "4. `model.add(keras.layers.Dense(1, activation='sigmoid'))`: This line adds the output layer to the model. It is a single neuron layer since this is a binary classification problem. The `activation='sigmoid'` argument sets the sigmoid activation function, which squashes the output between 0 and 1, representing the probability of the positive class. The output layer provides the final prediction of the model.\n",
    "\n",
    "Each `Dense` layer in the model is fully connected, meaning each neuron in a layer is connected to every neuron in the previous layer. The number of units in each layer determines the complexity and expressive power of the model. The activation functions introduce non-linearities into the model, allowing it to learn complex relationships between features and make accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "224dbe3d",
   "metadata": {},
   "source": [
    "#### Step 4: Compile the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25f72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79415222",
   "metadata": {},
   "source": [
    "The `compile` method in Keras is used to configure the model for training :\n",
    "\n",
    "1. `loss='binary_crossentropy'`: The `loss` argument specifies the loss function to be used during training. In this case, `'binary_crossentropy'` is used as the loss function. It is commonly used for binary classification problems, where the goal is to minimize the binary cross-entropy between the true labels and the predicted probabilities. The binary cross-entropy loss is well-suited for problems with two classes.\n",
    "\n",
    "2. `optimizer='adam'`: The `optimizer` argument specifies the optimization algorithm to be used during training. In this case, `'adam'` is used as the optimizer. Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm that adjusts the learning rate adaptively based on the gradients of the model parameters. It combines the advantages of two other optimization methods, AdaGrad and RMSProp, to achieve fast convergence and handle sparse gradients efficiently.\n",
    "\n",
    "3. `metrics=['accuracy']`: The `metrics` argument specifies the evaluation metric(s) to be used during training and testing. Here, `['accuracy']` is provided as the metric. The accuracy metric measures the fraction of correctly predicted samples compared to the total number of samples. It is a commonly used metric for classification tasks, providing an intuitive understanding of the model's performance in terms of correct predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caf679c8",
   "metadata": {},
   "source": [
    "#### Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83ad0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "46/46 [==============================] - 1s 3ms/step - loss: 0.4785 - accuracy: 0.7714\n",
      "Epoch 2/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9407\n",
      "Epoch 3/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.9582\n",
      "Epoch 4/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9714\n",
      "Epoch 5/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.9736\n",
      "Epoch 6/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.9758\n",
      "Epoch 7/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9802\n",
      "Epoch 8/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9824\n",
      "Epoch 9/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9802\n",
      "Epoch 10/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0599 - accuracy: 0.9824\n",
      "Epoch 11/50\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9824\n",
      "Epoch 12/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9846\n",
      "Epoch 13/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9846\n",
      "Epoch 14/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9868\n",
      "Epoch 15/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9890\n",
      "Epoch 16/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9868\n",
      "Epoch 17/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9890\n",
      "Epoch 18/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9912\n",
      "Epoch 19/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.9912\n",
      "Epoch 20/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9912\n",
      "Epoch 21/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9912\n",
      "Epoch 22/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9912\n",
      "Epoch 23/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9912\n",
      "Epoch 24/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9934\n",
      "Epoch 25/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9956\n",
      "Epoch 26/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9934\n",
      "Epoch 27/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9956\n",
      "Epoch 28/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9956\n",
      "Epoch 29/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9978\n",
      "Epoch 30/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9978\n",
      "Epoch 31/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9978\n",
      "Epoch 32/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9978\n",
      "Epoch 33/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9978\n",
      "Epoch 34/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9978\n",
      "Epoch 35/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9978\n",
      "Epoch 36/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9978\n",
      "Epoch 37/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 38/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 39/50\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 41/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9978\n",
      "Epoch 43/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1ae8472e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for 50 epochs\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f78b83e",
   "metadata": {},
   "source": [
    "The `fit` method in Keras is used to train the model on a given dataset:\n",
    "\n",
    "1. `X_train` and `y_train`: These arguments represent the training data.\n",
    "`X_train` refers to the input features (often represented as a NumPy array or a Pandas DataFrame), and\n",
    "`y_train` refers to the corresponding target labels (the expected outputs). The model will be trained to learn patterns and relationships between the input features and the target labels.\n",
    "\n",
    "2. `epochs=50`: The `epochs` argument specifies the number of times the model will iterate over the entire training dataset. In this case, the model will be trained for 50 epochs. One epoch is defined as a complete pass through the entire training dataset.\n",
    "\n",
    "3. `batch_size=10`: The `batch_size` argument specifies the number of samples that will be propagated through the model at once. In each epoch, the training dataset is divided into multiple batches, and the model's parameters are updated after each batch. A smaller batch size allows for more frequent updates of the model's parameters but can increase the training time. Here, a batch size of 10 is set.\n",
    "\n",
    "During training, the model will perform forward propagation to compute predictions based on the input data, and then backpropagation to calculate the gradients and update the model's parameters (weights and biases) using the optimization algorithm specified during compilation.\n",
    "\n",
    "The model will aim to minimize the loss function specified during compilation, and it will try to maximize the specified metrics to improve its performance. The training process continues for the specified number of epochs or until a convergence criterion is met.\n",
    "\n",
    "After training, the model will have learned the patterns and relationships in the training data and will be able to make predictions on new, unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba4c4a09",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0dd2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9737\n",
      "Test Loss: 0.08829933404922485\n",
      "Test Accuracy: 0.9736841917037964\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "score = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d55a5d6",
   "metadata": {},
   "source": [
    "The `evaluate` method in Keras is used to evaluate the trained model on a separate test dataset:\n",
    "\n",
    "1. `score = model.evaluate(X_test, y_test)`: This line evaluates the trained model on the test dataset. The `X_test` represents the input features of the test dataset, and `y_test` represents the corresponding target labels. The model will make predictions on the test data and compare them with the true labels to compute the evaluation metrics specified during model compilation (in this case, the loss function and accuracy). The `evaluate` method returns the calculated evaluation metrics.\n",
    "\n",
    "2. `print(\"Test Loss:\", score[0])`: This line prints the test loss, which is the value of the loss function computed on the test dataset. The loss function measures the discrepancy between the predicted outputs and the true labels. A lower test loss indicates better performance of the model on the unseen data.\n",
    "\n",
    "3. `print(\"Test Accuracy:\", score[1])`: This line prints the test accuracy, which is the value of the accuracy metric computed on the test dataset. The accuracy metric represents the percentage of correctly predicted samples out of the total number of samples in the test dataset. A higher test accuracy indicates a better ability of the model to make accurate predictions on unseen data.\n",
    "\n",
    "By evaluating the model on the test data, you can assess its performance and determine how well it generalizes to new, unseen samples. This evaluation helps you understand how the model is likely to perform in real-world scenarios and can guide decisions on model selection and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
